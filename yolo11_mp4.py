from ultralytics import YOLO
import cv2
import json
import numpy as np
import datetime
import os
from pathlib import Path

class VideoPoseAnalyzer:
    def __init__(self, model_path='yolo11n-pose.pt'):
        # YOLO11 í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ
        self.model = YOLO(model_path)
        
        # ì €ì¥ í´ë” ìƒì„±
        self.save_dir = "video_pose_outputs"
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)
        
        # ê´€ì ˆ ì´ë¦„ ì •ì˜
        self.keypoint_names = [
            'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',
            'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
            'left_wrist', 'right_wrist', 'left_hip', 'right_hip',
            'left_knee', 'right_knee', 'left_ankle', 'right_ankle'
        ]
    
    def analyze_video(self, video_path, save_output=True, show_video=True):
        """
        ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ í¬ì¦ˆ ë¶„ì„
        
        Args:
            video_path: ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            save_output: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            show_video: ì‹¤ì‹œê°„ í™”ë©´ í‘œì‹œ ì—¬ë¶€
        """
        # ë¹„ë””ì˜¤ íŒŒì¼ ì²´í¬
        if not os.path.exists(video_path):
            print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return None
        
        # ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´ ìƒì„±
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return None
        
        # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì •ë³´:")
        print(f"   - íŒŒì¼: {Path(video_path).name}")
        print(f"   - í•´ìƒë„: {width}x{height}")
        print(f"   - FPS: {fps}")
        print(f"   - ì´ í”„ë ˆì„: {total_frames}")
        print(f"   - ì¬ìƒì‹œê°„: {duration:.2f}ì´ˆ")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        video_name = Path(video_path).stem
        
        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
        output_video_path = None
        video_writer = None
        if save_output:
            output_video_path = f'{self.save_dir}/{video_name}_pose_output_{timestamp}.avi'
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
        
        # ë°ì´í„° ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        pose_data = []
        frame_count = 0
        
        print(f"\nğŸš€ í¬ì¦ˆ ë¶„ì„ ì‹œì‘...")
        print(f"   ESC: ì¤‘ë‹¨, SPACE: ì¼ì‹œì •ì§€/ì¬ìƒ")
        
        paused = False
        
        while True:
            if not paused:
                ret, frame = cap.read()
                if not ret:
                    print("âœ… ë¹„ë””ì˜¤ ë¶„ì„ ì™„ë£Œ!")
                    break
                
                # í¬ì¦ˆ ì¶”ì • ì‹¤í–‰
                results = self.model(frame, verbose=False)
                
                # ê²°ê³¼ ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥
                frame_data = self.extract_pose_data(results, frame_count, frame_count/fps)
                if frame_data:
                    pose_data.append(frame_data)
                
                # ê²°ê³¼ë¥¼ í”„ë ˆì„ì— ê·¸ë¦¬ê¸°
                annotated_frame = results[0].plot()
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress = (frame_count + 1) / total_frames * 100
                time_current = frame_count / fps
                
                # ì •ë³´ í…ìŠ¤íŠ¸ ì¶”ê°€
                cv2.putText(annotated_frame, f'Frame: {frame_count+1}/{total_frames}', 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(annotated_frame, f'Progress: {progress:.1f}%', 
                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(annotated_frame, f'Time: {time_current:.2f}s / {duration:.2f}s', 
                           (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                if results[0].keypoints is not None:
                    num_people = len(results[0].keypoints.xy)
                    cv2.putText(annotated_frame, f'People: {num_people}', 
                               (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # ì¶œë ¥ ë¹„ë””ì˜¤ì— ì €ì¥
                if save_output and video_writer is not None:
                    video_writer.write(annotated_frame)
                
                frame_count += 1
            else:
                # ì¼ì‹œì •ì§€ ì¤‘ì¼ ë•ŒëŠ” ê°™ì€ í”„ë ˆì„ í‘œì‹œ
                annotated_frame = results[0].plot() if 'results' in locals() else frame
                cv2.putText(annotated_frame, 'PAUSED - Press SPACE to continue', 
                           (10, height-30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            # í™”ë©´ì— ì¶œë ¥
            if show_video:
                cv2.imshow(f'YOLO11 Pose Analysis - {Path(video_path).name}', annotated_frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(30) & 0xFF
                if key == 27:  # ESC
                    print("âŒ ì‚¬ìš©ìê°€ ë¶„ì„ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                    break
                elif key == ord(' '):  # SPACE
                    paused = not paused
                    print("â¸ï¸ ì¼ì‹œì •ì§€" if paused else "â–¶ï¸ ì¬ìƒ")
            else:
                # í™”ë©´ ì¶œë ¥ ì—†ì´ ë¹ ë¥¸ ì²˜ë¦¬
                if frame_count % 30 == 0:  # 30í”„ë ˆì„ë§ˆë‹¤ ì§„í–‰ë¥  ì¶œë ¥
                    print(f"ì§„í–‰ë¥ : {progress:.1f}% ({frame_count+1}/{total_frames})")
        
        # ì •ë¦¬
        cap.release()
        if video_writer is not None:
            video_writer.release()
        if show_video:
            cv2.destroyAllWindows()
        
        # ê²°ê³¼ ì €ì¥
        if save_output and pose_data:
            self.save_results(pose_data, video_name, timestamp, video_path)
        
        return {
            'pose_data': pose_data,
            'video_info': {
                'fps': fps,
                'width': width,
                'height': height,
                'total_frames': total_frames,
                'duration': duration,
                'analyzed_frames': len(pose_data)
            },
            'output_video': output_video_path
        }
    
    def extract_pose_data(self, results, frame_number, timestamp_sec):
        """í¬ì¦ˆ ë°ì´í„° ì¶”ì¶œ"""
        if results[0].keypoints is None:
            return None
        
        # í‚¤í¬ì¸íŠ¸ ë°ì´í„° ì¶”ì¶œ
        keypoints_xy = results[0].keypoints.xy.cpu().numpy()
        keypoints_conf = results[0].keypoints.conf.cpu().numpy()
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ
        boxes = None
        box_conf = None
        if results[0].boxes is not None:
            boxes = results[0].boxes.xyxy.cpu().numpy()
            box_conf = results[0].boxes.conf.cpu().numpy()
        
        # í”„ë ˆì„ ë°ì´í„° êµ¬ì„±
        frame_data = {
            'frame_number': frame_number,
            'timestamp_sec': timestamp_sec,
            'num_people': len(keypoints_xy),
            'people': []
        }
        
        # ê° ì‚¬ëŒë³„ ë°ì´í„° ì €ì¥
        for person_idx in range(len(keypoints_xy)):
            person_data = {
                'person_id': person_idx,
                'keypoints': {
                    'coordinates': keypoints_xy[person_idx].tolist(),
                    'confidence': keypoints_conf[person_idx].tolist(),
                    'named_points': {}
                },
                'bounding_box': {
                    'coordinates': boxes[person_idx].tolist() if boxes is not None else None,
                    'confidence': float(box_conf[person_idx]) if box_conf is not None else None
                }
            }
            
            # ê´€ì ˆë³„ ìƒì„¸ ì •ë³´
            for i, name in enumerate(self.keypoint_names):
                person_data['keypoints']['named_points'][name] = {
                    'x': float(keypoints_xy[person_idx][i][0]),
                    'y': float(keypoints_xy[person_idx][i][1]),
                    'confidence': float(keypoints_conf[person_idx][i]),
                    'visible': bool(keypoints_conf[person_idx][i] > 0.5)
                }
            
            frame_data['people'].append(person_data)
        
        return frame_data
    
    def save_results(self, pose_data, video_name, timestamp, original_video_path):
        """ê²°ê³¼ ì €ì¥"""
        # JSON ì €ì¥
        json_path = f'{self.save_dir}/{video_name}_pose_data_{timestamp}.json'
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        output_data = {
            'metadata': {
                'original_video': original_video_path,
                'analysis_timestamp': datetime.datetime.now().isoformat(),
                'total_frames_analyzed': len(pose_data),
                'model_used': 'yolo11n-pose'
            },
            'frames': pose_data
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        # CSV ì €ì¥ (ì²« ë²ˆì§¸ ì‚¬ëŒë§Œ)
        csv_data = []
        for frame in pose_data:
            if frame['people']:
                person = frame['people'][0]
                row = {
                    'frame': frame['frame_number'],
                    'timestamp_sec': frame['timestamp_sec']
                }
                for name, point in person['keypoints']['named_points'].items():
                    row[f'{name}_x'] = point['x']
                    row[f'{name}_y'] = point['y']
                    row[f'{name}_conf'] = point['confidence']
                csv_data.append(row)
        
        if csv_data:
            import pandas as pd
            df = pd.DataFrame(csv_data)
            csv_path = f'{self.save_dir}/{video_name}_pose_data_{timestamp}.csv'
            df.to_csv(csv_path, index=False)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   JSON: {json_path}")
        if csv_data:
            print(f"   CSV:  {csv_path}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = VideoPoseAnalyzer('yolo11n-pose.pt')
    
    # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ì—¬ê¸°ì— ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì…ë ¥)
    video_files = [
        "dance_video.mp4",      # ëŒ„ìŠ¤ ì˜ìƒ
        "sample_video.avi",     # ìƒ˜í”Œ ì˜ìƒ
        "test.mov",            # í…ŒìŠ¤íŠ¸ ì˜ìƒ
    ]
    
    print("ğŸ¯ YOLO11 ë¹„ë””ì˜¤ í¬ì¦ˆ ë¶„ì„ê¸°")
    print("=" * 50)
    
    # ì‚¬ìš©ìì—ê²Œ íŒŒì¼ ì„ íƒí•˜ê²Œ í•˜ê¸°
    print("ë¶„ì„í•  ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    print("(ì˜ˆ: dance_video.mp4, /path/to/video.mp4)")
    video_path = input("ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ: ").strip()
    
    if not video_path:
        print("âŒ íŒŒì¼ ê²½ë¡œê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        # ë¹„ë””ì˜¤ ë¶„ì„ ì‹¤í–‰
        results = analyzer.analyze_video(
            video_path=video_path,
            save_output=True,    # ê²°ê³¼ ì €ì¥
            show_video=True      # ì‹¤ì‹œê°„ í™”ë©´ í‘œì‹œ
        )
        
        if results:
            print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(f"   - ì´ í”„ë ˆì„: {results['video_info']['total_frames']}")
            print(f"   - ë¶„ì„ëœ í”„ë ˆì„: {results['video_info']['analyzed_frames']}")
            print(f"   - ë¹„ë””ì˜¤ ê¸¸ì´: {results['video_info']['duration']:.2f}ì´ˆ")
            print(f"   - FPS: {results['video_info']['fps']}")
            
            # í¬ì¦ˆê°€ ê²€ì¶œëœ í”„ë ˆì„ ë¹„ìœ¨ ê³„ì‚°
            pose_frames = len(results['pose_data'])
            total_frames = results['video_info']['analyzed_frames']
            if total_frames > 0:
                detection_rate = pose_frames / total_frames * 100
                print(f"   - í¬ì¦ˆ ê²€ì¶œë¥ : {detection_rate:.1f}%")
        else:
            print("âŒ ë¹„ë””ì˜¤ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")